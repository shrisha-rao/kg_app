#src/processing/embedding/local.py
import logging
from typing import List, Optional
from sentence_transformers import SentenceTransformer
from .base import EmbeddingGenerator
from src.config import settings


class LocalEmbeddingGenerator(EmbeddingGenerator):
    """Local embedding generator using Sentence Transformers."""

    def __init__(self, model_name: Optional[str] = None):
        self.model_name = model_name or settings.local_embedding_model
        self.logger = logging.getLogger(__name__)
        self.model = self._load_model()
        self.embedding_dimension = self.model.get_sentence_embedding_dimension(
        )

    def _load_model(self) -> SentenceTransformer:
        """Load the Sentence Transformers model."""
        try:
            self.logger.info(
                f"Loading local embedding model: {self.model_name}")
            return SentenceTransformer(self.model_name)
        except Exception as e:
            self.logger.error(
                f"Failed to load model {self.model_name}: {str(e)}")
            # Fallback to a default model
            self.logger.info(
                f"Loading default model: {settings.local_embedding_model}")
            # all-MiniLM-L6-v2
            return SentenceTransformer(settings.local_embedding_model)
            #"all-mpnet-base-v2")
            #"all-MiniLM-L6-v2"):

    def generate_embedding(self, text: str) -> List[float]:
        """Generate embedding for a single text string."""
        if not text or not text.strip():
            return [0.0] * self.embedding_dimension

        try:
            embedding = self.model.encode(text)
            return embedding.tolist()
        except Exception as e:
            self.logger.error(f"Error generating embedding: {str(e)}")
            return [0.0] * self.embedding_dimension

    def generate_embeddings_batch(self, texts: List[str]) -> List[List[float]]:
        """Generate embeddings for a batch of text strings."""
        if not texts:
            return []

        # Filter out empty texts
        non_empty_texts = [text for text in texts if text and text.strip()]
        empty_indices = [
            i for i, text in enumerate(texts) if not text or not text.strip()
        ]

        if not non_empty_texts:
            return [[0.0] * self.embedding_dimension for _ in texts]

        try:
            embeddings = self.model.encode(non_empty_texts)
            result = embeddings.tolist()

            # Insert zero vectors for empty texts
            for idx in empty_indices:
                result.insert(idx, [0.0] * self.embedding_dimension)

            return result
        except Exception as e:
            self.logger.error(f"Error generating batch embeddings: {str(e)}")
            return [[0.0] * self.embedding_dimension for _ in texts]

    def get_embedding_dimension(self) -> int:
        """Get the dimension of the embeddings generated by this model."""
        return self.embedding_dimension
