# src/services/llm/local_llm.py
import logging
import os
import aiohttp
from typing import Dict, List, Optional, Any
from sentence_transformers import SentenceTransformer
from .base import LLMService, LLMResponse
from .mock_llm import MockLLMService

logger = logging.getLogger(__name__)


class LocalLLMService(LLMService):
    """Local LLM service using sentence-transformers for embeddings + Ollama for text"""

    def __init__(self, model_name: str = "all-MiniLM-L6-v2"):
        logger.info(f"Initializing Local LLM Service with model: {model_name}")
        self.model = SentenceTransformer(model_name)
        self.embedding_dimension = self.model.get_sentence_embedding_dimension(
        )
        # Ollama config
        self.use_ollama = os.getenv("USE_OLLAMA", "false").lower() == "true"
        self.ollama_model = os.getenv("OLLAMA_MODEL", "tinyllama")
        self.ollama_host = os.getenv("OLLAMA_HOST", "http://ollama:11434")

        if self.use_ollama:
            logger.info(f"Ollama enabled with model: {self.ollama_model}")
        else:
            logger.info("Using mock text generation (Ollama disabled)")

    async def generate_embeddings(
            self,
            texts: List[str],
            model: Optional[str] = None) -> List[List[float]]:
        """Generate embeddings using local model"""
        logger.info(f"Local: Generating embeddings for {len(texts)} texts")
        embeddings = self.model.encode(texts).tolist()
        logger.info(
            f"Local: Generated {len(embeddings)} embeddings of dimension {len(embeddings[0])}"
        )
        return embeddings

    async def generate_response(self,
                                prompt: str,
                                context: Optional[str] = None,
                                temperature: float = 0.2,
                                max_tokens: int = 1024,
                                **kwargs) -> LLMResponse:
        """Generate response using Ollama or mock"""
        logger.info(
            f"Local: Generating response for prompt: {prompt[:100]}...")

        if self.use_ollama:
            return await self._generate_ollama_response(
                prompt, temperature, max_tokens)
        else:
            return await self._generate_mock_response(prompt)

    async def _generate_ollama_response(self, prompt: str, temperature: float,
                                        max_tokens: int) -> LLMResponse:
        """Generate response using Ollama"""
        try:
            async with aiohttp.ClientSession() as session:
                payload = {
                    "model": self.ollama_model,
                    "prompt": prompt,
                    "stream": False,
                    "options": {
                        "temperature": temperature,
                        "num_predict": max_tokens
                    }
                }
                async with session.post(f"{self.ollama_host}/api/generate",
                                        json=payload) as resp:
                    result = await resp.json()
                    return LLMResponse(content=result.get("response", ""),
                                       reasoning="Generated by Ollama",
                                       confidence=0.8,
                                       tokens_used=result.get("eval_count", 0),
                                       model=self.ollama_model,
                                       metadata={"ollama_response": True})
        except Exception as e:
            logger.error(f"Ollama error: {e}, falling back to mock")
            return await self._generate_mock_response(prompt)

    async def _generate_mock_response(self, prompt: str) -> LLMResponse:
        """Generate mock response"""
        response_text = f"Local mock response to: {prompt[:50]}... (Embeddings were generated locally)"
        return LLMResponse(
            content=response_text,
            reasoning="This is a mock response from local LLM service",
            confidence=0.8,
            tokens_used=len(response_text.split()),
            model="local-mock",
            metadata={"embedding_model": self.embedding_dimension})

    # async def generate_structured_response(self,
    #                                        prompt: str,
    #                                        response_format: Dict[str, Any],
    #                                        context: Optional[str] = None,
    #                                        temperature: float = 0.1,
    #                                        max_tokens: int = 1024,
    #                                        **kwargs) -> Dict[str, Any]:
    #     """Generate structured response using Ollama or mock"""
    #     if self.use_ollama:
    #         # For now, use mock for structured responses with Ollama
    #         # Could be enhanced to use Ollama's JSON mode
    #         return await MockLLMService().generate_structured_response(
    #             prompt, response_format, context, temperature, max_tokens,
    #             **kwargs)
    #     else:
    #         return await MockLLMService().generate_structured_response(
    #             prompt, response_format, context, temperature, max_tokens,
    #             **kwargs)

    async def get_available_models(self) -> List[str]:
        """Get available local models"""
        return ["all-MiniLM-L6-v2", "paraphrase-MiniLM-L3-v2"]

    async def generate_structured_response(self,
                                           prompt: str,
                                           response_format: Dict[str, Any],
                                           context: Optional[str] = None,
                                           temperature: float = 0.1,
                                           max_tokens: int = 1024,
                                           **kwargs) -> Dict[str, Any]:
        logger.info(f"USE_OLLAMA={self.use_ollama}")
        if self.use_ollama:
            # Use Ollama for structured responses
            return await self._generate_ollama_structured_response(
                prompt, response_format, context, temperature, max_tokens,
                **kwargs)
        else:
            return await MockLLMService().generate_structured_response(
                prompt, response_format, context, temperature, max_tokens,
                **kwargs)

    async def _generate_ollama_structured_response(self,
                                                   prompt,
                                                   response_format,
                                                   context=None,
                                                   temperature=0.1,
                                                   max_tokens=1024,
                                                   **kwargs):
        logger.info("Using Ollama for structured response")  # Add this
        # structured_prompt = f"""{prompt}
        # Please respond in JSON format with the following structure:
        # {response_format}"""

        structured_prompt = f"""{prompt}

        You MUST respond with ONLY valid JSON, no other text. The JSON must have this exact structure:
        {json.dumps(response_format, indent=2)}

        JSON response:"""

        response = await self._generate_ollama_response(
            structured_prompt, temperature, max_tokens)
        logger.info(f"Ollama raw response: {response.content}")  # Add this

        try:
            import json
            result = json.loads(response.content)
            logger.info(f"Parsed JSON: {result}")  # Add this
            return result
        except Exception as e:
            logger.error(f"JSON parse failed: {e}")  # Add this
            return await MockLLMService().generate_structured_response(
                prompt, response_format, context, temperature, max_tokens,
                **kwargs)


# # src/services/llm/local_llm.py
# import logging
# from typing import Dict, List, Optional, Any
# from sentence_transformers import SentenceTransformer
# from .base import LLMService, LLMResponse

# ####################################

# from .mock_llm import MockLLMService

# ####################################

# logger = logging.getLogger(__name__)

# class LocalLLMService(LLMService):
#     """Local LLM service using sentence-transformers for embeddings"""

#     def __init__(self,
#                  model_name: str = "all-mpnet-base-v2"):  #"all-MiniLM-L6-v2"):
#         logger.info(f"Initializing Local LLM Service with model: {model_name}")
#         self.model = SentenceTransformer(model_name)
#         self.embedding_dimension = self.model.get_sentence_embedding_dimension(
#         )

#     async def generate_embeddings(
#             self,
#             texts: List[str],
#             model: Optional[str] = None) -> List[List[float]]:
#         """Generate embeddings using local model"""
#         logger.info(f"Local: Generating embeddings for {len(texts)} texts")
#         embeddings = self.model.encode(texts).tolist()
#         logger.info(
#             f"Local: Generated {len(embeddings)} embeddings of dimension {len(embeddings[0])}"
#         )
#         return embeddings

#     # For text generation, we can't easily run local LLMs, so use mock responses
#     async def generate_response(self,
#                                 prompt: str,
#                                 context: Optional[str] = None,
#                                 temperature: float = 0.2,
#                                 max_tokens: int = 1024,
#                                 **kwargs) -> LLMResponse:
#         """Generate mock response since we don't have a local text generation model"""
#         logger.info(
#             f"Local: Generating mock response for prompt: {prompt[:100]}...")

#         response_text = f"Local mock response to: {prompt[:50]}... (Embeddings were generated locally)"

#         return LLMResponse(
#             content=response_text,
#             reasoning="This is a mock response from local LLM service",
#             confidence=0.8,
#             tokens_used=len(response_text.split()),
#             model="local-mock",
#             metadata={
#                 "embedding_model":
#                 self.model.get_sentence_embedding_dimension()
#             })

#     async def generate_structured_response(self,
#                                            prompt: str,
#                                            response_format: Dict[str, Any],
#                                            context: Optional[str] = None,
#                                            temperature: float = 0.1,
#                                            max_tokens: int = 1024,
#                                            **kwargs) -> Dict[str, Any]:
#         """Generate mock structured response"""
#         return await MockLLMService().generate_structured_response(
#             prompt, response_format, context, temperature, max_tokens,
#             **kwargs)

#     async def get_available_models(self) -> List[str]:
#         """Get available local models"""
#         return [
#             "all-MiniLM-L6-v2", "paraphrase-MiniLM-L3-v2", "all-mpnet-base-v2"
#         ]
