{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42d4231-a294-417f-9ef1-3a6e686e3538",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6433b15b-1855-4c42-9868-66b1cf678312",
   "metadata": {},
   "outputs": [],
   "source": [
    "# src/processing/ner_extractor.py\n",
    "import logging\n",
    "import json\n",
    "from typing import List, Tuple, Dict, Any\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "\n",
    "from src.models.paper import Entity, Relation\n",
    "from src.config import settings\n",
    "from src.services.llm import get_llm_service\n",
    "#import pdb\n",
    "\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from spacy.tokens import Span\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "import pdb\n",
    "\n",
    "\n",
    "class NERExtractor:\n",
    "    \"\"\"Base class for NER and relation extraction\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.extraction_method = settings.ner_extraction_method  # \"spacy\" or \"llm\"\n",
    "\n",
    "    async def extract_entities_and_relations(\n",
    "            self, text: str) -> Tuple[List[Entity], List[Relation]]:\n",
    "        \"\"\"Extract entities and relations from text\"\"\"\n",
    "        if self.extraction_method == \"spacy\":\n",
    "            return await self._extract_with_spacy(text)\n",
    "        else:\n",
    "            return await self._extract_with_llm(text)\n",
    "\n",
    "    async def _extract_with_spacy(\n",
    "            self, text: str) -> Tuple[List[Entity], List[Relation]]:\n",
    "        \"\"\"Extract entities and relations using spaCy\"\"\"\n",
    "        try:\n",
    "            # Import spacy (we'll handle the import here to avoid dependency issues if not using spacy)\n",
    "            # import spacy\n",
    "            # from spacy import displacy\n",
    "            # from spacy.tokens import Span\n",
    "            # from spacy.matcher import Matcher\n",
    "\n",
    "            print(\" ----------------> bp 1\")\n",
    "\n",
    "            \n",
    "\n",
    "            # Load the appropriate spaCy model\n",
    "            try:\n",
    "                nlp = spacy.load(\"en_core_web_sm\")\n",
    "            except OSError:\n",
    "                # If model is not available, download it\n",
    "                import subprocess\n",
    "                subprocess.run(\n",
    "                    [\"python\", \"-m\", \"spacy\", \"download\", \"en_core_web_sm\"])\n",
    "                nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "            pdb.set_trace()  # Set a breakpoint here\n",
    "            # Process the text\n",
    "            doc = nlp(text)\n",
    "\n",
    "            print(\" ----------------> bp 2\")\n",
    "\n",
    "            entities = []\n",
    "            relations = []\n",
    "\n",
    "            # Extract entities\n",
    "            for ent in doc.ents:\n",
    "                # Map spaCy entity types to our types\n",
    "                entity_type = self._map_spacy_entity_type(ent.label_)\n",
    "                if entity_type:\n",
    "                    entities.append(\n",
    "                        Entity(\n",
    "                            text=ent.text,\n",
    "                            type=entity_type,\n",
    "                            confidence=\n",
    "                            1.0  # spaCy doesn't provide confidence scores\n",
    "                        ))\n",
    "\n",
    "            # pdb.set_trace()  # Set a breakpoint here\n",
    "            # Extract relations using pattern matching\n",
    "            relations = self._extract_relations_with_patterns(doc)\n",
    "\n",
    "            logger.info(\n",
    "                f\"Extracted {len(entities)} entities and {len(relations)} relations with spaCy\"\n",
    "            )\n",
    "            return entities, relations\n",
    "\n",
    "        except ImportError:\n",
    "            logger.error(\n",
    "                \"spaCy is not installed. Please install it with: pip install spacy\"\n",
    "            )\n",
    "            return [], []\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in spaCy extraction: {e}\")\n",
    "            return [], []\n",
    "\n",
    "    def _map_spacy_entity_type(self, spacy_type: str) -> str:\n",
    "        \"\"\"Map spaCy entity types to our entity types\"\"\"\n",
    "        mapping = {\n",
    "            \"PERSON\": \"person\",\n",
    "            \"ORG\": \"organization\",\n",
    "            \"GPE\": \"location\",\n",
    "            \"LOC\": \"location\",\n",
    "            \"FAC\": \"location\",\n",
    "            \"PRODUCT\": \"concept\",\n",
    "            \"EVENT\": \"concept\",\n",
    "            \"WORK_OF_ART\": \"concept\",\n",
    "            \"LAW\": \"concept\",\n",
    "            \"LANGUAGE\": \"concept\",\n",
    "            \"DATE\": None,  # We don't want dates\n",
    "            \"TIME\": None,  # We don't want times\n",
    "            \"PERCENT\": None,  # We don't want percentages\n",
    "            \"MONEY\": None,  # We don't want monetary values\n",
    "            \"QUANTITY\": None,  # We don't want quantities\n",
    "            \"ORDINAL\": None,  # We don't want ordinals\n",
    "            \"CARDINAL\": None,  # We don't want cardinals\n",
    "        }\n",
    "        return mapping.get(spacy_type, \"concept\")  # Default to concept\n",
    "\n",
    "    def _extract_relations_with_patterns(self, doc) -> List[Relation]:\n",
    "        \"\"\"Extract relations using pattern matching\"\"\"\n",
    "        relations = []\n",
    "\n",
    "        # Define patterns for common relations in research papers\n",
    "        patterns = [\n",
    "            # Pattern for \"method X is used for Y\"\n",
    "            [{\n",
    "                \"POS\": \"NOUN\",\n",
    "                \"OP\": \"*\"\n",
    "            }, {\n",
    "                \"POS\": \"VERB\",\n",
    "                \"OP\": \"*\"\n",
    "            }, {\n",
    "                \"POS\": \"ADP\",\n",
    "                \"OP\": \"*\"\n",
    "            }, {\n",
    "                \"POS\": \"NOUN\",\n",
    "                \"OP\": \"+\"\n",
    "            }],\n",
    "            # Pattern for \"X et al. proposed Y\"\n",
    "            [{\n",
    "                \"ENT_TYPE\": \"PERSON\",\n",
    "                \"OP\": \"+\"\n",
    "            }, {\n",
    "                \"LOWER\": \"et\",\n",
    "                \"OP\": \"*\"\n",
    "            }, {\n",
    "                \"LOWER\": \"al\",\n",
    "                \"OP\": \"*\"\n",
    "            }, {\n",
    "                \"LOWER\": \".\",\n",
    "                \"OP\": \"*\"\n",
    "            }, {\n",
    "                \"POS\": \"VERB\",\n",
    "                \"OP\": \"+\"\n",
    "            }, {\n",
    "                \"POS\": \"NOUN\",\n",
    "                \"OP\": \"+\"\n",
    "            }],\n",
    "            # Pattern for \"X is based on Y\"\n",
    "            [{\n",
    "                \"POS\": \"NOUN\",\n",
    "                \"OP\": \"+\"\n",
    "            }, {\n",
    "                \"LOWER\": \"is\"\n",
    "            }, {\n",
    "                \"LOWER\": \"based\"\n",
    "            }, {\n",
    "                \"LOWER\": \"on\"\n",
    "            }, {\n",
    "                \"POS\": \"NOUN\",\n",
    "                \"OP\": \"+\"\n",
    "            }]\n",
    "        ]\n",
    "        pdb.set_trace()\n",
    "        # Create a matcher\n",
    "        #nlp = doc._.__class__  # Get the nlp object from the doc\n",
    "        #nlp = doc.__class__  # Get the nlp object from the doc\n",
    "        #nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "        matcher = Matcher(doc.vocab)\n",
    "\n",
    "        for i, pattern in enumerate(patterns):\n",
    "            matcher.add(f\"RELATION_{i}\", [pattern])\n",
    "\n",
    "        matches = matcher(doc)\n",
    "\n",
    "        for match_id, start, end in matches:\n",
    "            span = doc[start:end]\n",
    "            # This is a simplified implementation\n",
    "            # In a real implementation, you would parse the span to extract specific entities and relations\n",
    "\n",
    "            # For now, we'll just create a generic relation\n",
    "            if len(span.ents) >= 2:\n",
    "                source = span.ents[0]\n",
    "                target = span.ents[1]\n",
    "\n",
    "                source_type = self._map_spacy_entity_type(source.label_)\n",
    "                target_type = self._map_spacy_entity_type(target.label_)\n",
    "\n",
    "                if source_type and target_type:\n",
    "                    relations.append(\n",
    "                        Relation(\n",
    "                            source_entity=Entity(text=source.text,\n",
    "                                                 type=source_type,\n",
    "                                                 confidence=1.0),\n",
    "                            target_entity=Entity(text=target.text,\n",
    "                                                 type=target_type,\n",
    "                                                 confidence=1.0),\n",
    "                            relationship=\"related_to\",\n",
    "                            confidence=\n",
    "                            0.7  # Default confidence for pattern-based relations\n",
    "                        ))\n",
    "\n",
    "        return relations\n",
    "\n",
    "    async def _extract_with_llm(\n",
    "            self, text: str) -> Tuple[List[Entity], List[Relation]]:\n",
    "        \"\"\"Extract entities and relations using LLM\"\"\"\n",
    "        try:\n",
    "            llm_service = get_llm_service()\n",
    "\n",
    "            # Create a prompt for the LLM to extract entities and relations\n",
    "            prompt = f\"\"\"\n",
    "            Extract named entities and their relationships from the following research paper text.\n",
    "            \n",
    "            Return the results in JSON format with two arrays: \"entities\" and \"relations\".\n",
    "            \n",
    "            For entities, include:\n",
    "            - text: the entity text\n",
    "            - type: one of [\"person\", \"organization\", \"location\", \"concept\", \"methodology\"]\n",
    "            - confidence: a confidence score between 0 and 1\n",
    "            \n",
    "            For relations, include:\n",
    "            - source: the source entity text\n",
    "            - target: the target entity text\n",
    "            - relationship: the type of relationship\n",
    "            - confidence: a confidence score between 0 and 1\n",
    "            \n",
    "            Text to analyze:\n",
    "            {text[:4000]}  # Limit text length to avoid token limits\n",
    "            \n",
    "            JSON Response:\n",
    "            \"\"\"\n",
    "\n",
    "            # Get response from LLM\n",
    "            response = await llm_service.generate_response(prompt,\n",
    "                                                           temperature=0.1,\n",
    "                                                           max_tokens=2000)\n",
    "\n",
    "            # Parse the JSON response\n",
    "            try:\n",
    "                # Extract JSON from the response (LLM might add some text before/after JSON)\n",
    "                json_start = response.content.find('{')\n",
    "                json_end = response.content.rfind('}') + 1\n",
    "                json_str = response.content[json_start:json_end]\n",
    "\n",
    "                data = json.loads(json_str)\n",
    "\n",
    "                # Convert to our models\n",
    "                entities = []\n",
    "                for entity_data in data.get(\"entities\", []):\n",
    "                    entities.append(\n",
    "                        Entity(text=entity_data.get(\"text\", \"\"),\n",
    "                               type=entity_data.get(\"type\", \"concept\"),\n",
    "                               confidence=entity_data.get(\"confidence\", 0.5)))\n",
    "\n",
    "                relations = []\n",
    "                for relation_data in data.get(\"relations\", []):\n",
    "                    # Find the source and target entities\n",
    "                    source_entity = next(\n",
    "                        (e for e in entities\n",
    "                         if e.text == relation_data.get(\"source\", \"\")), None)\n",
    "                    target_entity = next(\n",
    "                        (e for e in entities\n",
    "                         if e.text == relation_data.get(\"target\", \"\")), None)\n",
    "\n",
    "                    if source_entity and target_entity:\n",
    "                        relations.append(\n",
    "                            Relation(source_entity=source_entity,\n",
    "                                     target_entity=target_entity,\n",
    "                                     relationship=relation_data.get(\n",
    "                                         \"relationship\", \"related_to\"),\n",
    "                                     confidence=relation_data.get(\n",
    "                                         \"confidence\", 0.5)))\n",
    "\n",
    "                logger.info(\n",
    "                    f\"Extracted {len(entities)} entities and {len(relations)} relations with LLM\"\n",
    "                )\n",
    "                return entities, relations\n",
    "\n",
    "            except json.JSONDecodeError as e:\n",
    "                logger.error(f\"Failed to parse LLM response as JSON: {e}\")\n",
    "                logger.error(f\"LLM response: {response.content}\")\n",
    "                return [], []\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in LLM extraction: {e}\")\n",
    "            return [], []\n",
    "\n",
    "\n",
    "# Global instance\n",
    "ner_extractor = NERExtractor()\n",
    "\n",
    "\n",
    "# Public function to use the extractor\n",
    "async def extract_entities_and_relations(\n",
    "        text: str) -> Tuple[List[Entity], List[Relation]]:\n",
    "    \"\"\"\n",
    "    Extract entities and relations from text\n",
    "    \n",
    "    Args:\n",
    "        text: The text to extract from\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (entities, relations)\n",
    "    \"\"\"\n",
    "    return await ner_extractor.extract_entities_and_relations(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68ade06e-76f7-4e67-b0be-574e25829dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tests/unit/test_ner_extractor.py\n",
    "import sys\n",
    "import os\n",
    "import logging\n",
    "import asyncio\n",
    "from pathlib import Path\n",
    "\n",
    "# Add the src directory to the Python path\n",
    "#sys.path.insert(0, str(Path(__file__).parent.parent.parent))\n",
    "\n",
    "#from src.processing.ner_extractor import extract_entities_and_relations, NERExtractor\n",
    "\n",
    "# Set up basic logging\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "\n",
    "async def test_ner_extractor():\n",
    "    \"\"\"Test the NER extractor with sample research text\"\"\"\n",
    "\n",
    "    # Sample research paper text for testing\n",
    "    sample_text = \"\"\"\n",
    "    In their 2020 study, Dr. Jane Smith from Stanford University proposed a novel machine learning framework \n",
    "    called NeuroLearn for analyzing neurological data. The research was conducted in collaboration with \n",
    "    researchers at MIT and Harvard Medical School. The methodology combines convolutional neural networks \n",
    "    with attention mechanisms, achieving 95% accuracy on the Alzheimer's Disease dataset. \n",
    "    This approach significantly outperforms traditional methods like SVM and random forests.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Testing NER Extractor...\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Test spaCy extraction\n",
    "    print(\"\\nðŸ§ª Testing spaCy NER Extraction\")\n",
    "    extractor = NERExtractor()\n",
    "    extractor.extraction_method = \"spacy\"\n",
    "\n",
    "    try:\n",
    "        entities, relations = await extractor.extract_entities_and_relations(\n",
    "            sample_text)\n",
    "        print(f\"âœ… spaCy extraction successful\")\n",
    "        print(f\"ðŸ‘¤ Entities found: {len(entities)}\")\n",
    "        for entity in entities:\n",
    "            print(\n",
    "                f\"   - {entity.text} ({entity.type}, confidence: {entity.confidence})\"\n",
    "            )\n",
    "\n",
    "        print(f\"ðŸ”— Relations found: {len(relations)}\")\n",
    "        for relation in relations[:3]:  # Show first 3 relations\n",
    "            print(\n",
    "                f\"   - {relation.source_entity.text} â†’ {relation.target_entity.text} ({relation.relationship})\"\n",
    "            )\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ spaCy extraction failed: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a04e1c29-29a1-4c4a-b05c-a6cece672271",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing NER Extractor...\n",
      "==================================================\n",
      "\n",
      "ðŸ§ª Testing spaCy NER Extraction\n",
      " ----------------> bp 1\n",
      "> \u001b[0;32m/tmp/ipykernel_1813/2949211371.py\u001b[0m(65)\u001b[0;36m_extract_with_spacy\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     63 \u001b[0;31m            \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Set a breakpoint here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     64 \u001b[0;31m            \u001b[0;31m# Process the text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 65 \u001b[0;31m            \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     66 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     67 \u001b[0;31m            \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" ----------------> bp 2\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  c\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ----------------> bp 2\n",
      "> \u001b[0;32m/tmp/ipykernel_1813/2949211371.py\u001b[0m(188)\u001b[0;36m_extract_relations_with_patterns\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    186 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    187 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 188 \u001b[0;31m        \u001b[0mmatcher\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMatcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    189 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    190 \u001b[0;31m        \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpattern\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpatterns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/tmp/ipykernel_1813/2949211371.py\u001b[0m(190)\u001b[0;36m_extract_relations_with_patterns\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    188 \u001b[0;31m        \u001b[0mmatcher\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMatcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    189 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 190 \u001b[0;31m        \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpattern\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpatterns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    191 \u001b[0;31m            \u001b[0mmatcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"RELATION_{i}\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    192 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-23 14:05:03,120 - INFO - Extracted 6 entities and 0 relations with spaCy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… spaCy extraction successful\n",
      "ðŸ‘¤ Entities found: 6\n",
      "   - Jane Smith (person, confidence: 1.0)\n",
      "   - Stanford University (organization, confidence: 1.0)\n",
      "   - NeuroLearn (organization, confidence: 1.0)\n",
      "   - MIT (organization, confidence: 1.0)\n",
      "   - Harvard Medical School (organization, confidence: 1.0)\n",
      "   - SVM (location, confidence: 1.0)\n",
      "ðŸ”— Relations found: 0\n"
     ]
    }
   ],
   "source": [
    " await test_ner_extractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a8d025-3a6b-44ff-89e1-8bd4acfbca83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44213004-d2e5-4d84-ad5e-519f6730dc0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997eca0c-fb75-4b6e-9715-59fae5db9638",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef62ee8c-b3b8-420b-8cb0-792e93826ad1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbdf91a-e9fb-4140-8aa7-34c7c8d4f8f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d21ed12-933b-4c84-8848-cd9968c0d1eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
