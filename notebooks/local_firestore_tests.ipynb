{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5579bfa1-3cb8-4ff8-978a-fd9ea7aeb09d",
   "metadata": {},
   "source": [
    "**This comprehensive test notebook will:**\n",
    "- Test all CRUD operations - upsert, search, delete\n",
    "\n",
    "- Verify namespace functionality - separate data spaces\n",
    "\n",
    "- Check performance - batch operations and search speed\n",
    "\n",
    "- Handle edge cases - empty inputs, similar vectors, non-existent data\n",
    "\n",
    "- Provide detailed reporting - success/failure status for each test\n",
    "\n",
    "- Include cleanup - remove test data after tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e9c374b-7c9e-4c03-8bcc-ffcdc578ec3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.config:Loading ARANGODB_HOST: http://arangodb:8529\n",
      "INFO:src.config:Loading USE_MOCK_SERVICES: true\n",
      "INFO:src.services.vector_db.local_vector_db:Successfully connected to Firestore Emulator\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¥ Firestore Local Vector DB Test Suite\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# test_firestore_vector_db.ipynb\n",
    "import asyncio\n",
    "import logging\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# Add the project root to Python path\n",
    "sys.path.append('/app')\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Import the local vector DB service\n",
    "from src.services.vector_db.local_vector_db import LocalVectorDBService\n",
    "from src.config import settings\n",
    "\n",
    "# Initialize the service\n",
    "vector_db = LocalVectorDBService()\n",
    "\n",
    "print(\"ðŸ”¥ Firestore Local Vector DB Test Suite\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ebd80c8-5784-453d-be90-9ae2807dfb76",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Test 1: Basic Connection Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f62e2705-c088-4d46-bc3f-89085ce2614b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§ª Test 1: Testing Firestore Connection...\n",
      "âœ… Connection successful! Current stats: {'vectors_count': 0, 'namespaces': [], 'shards_count': 1, 'collection_name': 'vectors'}\n"
     ]
    }
   ],
   "source": [
    "# Test 1: Basic Connection and Collection Setup\n",
    "async def test_connection():\n",
    "    print(\"ðŸ§ª Test 1: Testing Firestore Connection...\")\n",
    "    try:\n",
    "        stats = await vector_db.get_index_stats()\n",
    "        print(f\"âœ… Connection successful! Current stats: {stats}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Connection failed: {e}\")\n",
    "        return False\n",
    "\n",
    "# Run test\n",
    "connection_ok = await test_connection()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caadc16f-703d-434e-9603-867336dffaa4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Test 2: Generate Sample Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d32223e3-bc0a-436f-87f4-0cd400956d30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Generating 5 sample embeddings...\n",
      "  âœ… Generated 5 embeddings, 5 IDs, 5 metadata entries\n",
      "Sample vector: [-0.031366072595119476, 0.014946346171200275, 0.004160337150096893, 0.06344591826200485, -0.01146529708057642]...\n",
      "Sample metadata: {'title': 'Machine Learning Research Paper #0', 'authors': ['Author_0', 'Researcher_0'], 'year': 2020, 'abstract': 'This is a test abstract for Machine Learning Research Paper - document 0', 'doc_id': 'doc_0', 'is_public': True}\n"
     ]
    }
   ],
   "source": [
    "# Test 2: Generate Sample Embeddings\n",
    "# CORRECTED: Generate Sample Embeddings Function\n",
    "def generate_sample_embeddings(num_vectors: int = 5, dimension: int = 384) -> tuple:\n",
    "    \"\"\"Generate sample embeddings for testing - FIXED VERSION\"\"\"\n",
    "    print(f\"  Generating {num_vectors} sample embeddings...\")\n",
    "    \n",
    "    # Generate random vectors (normalized for cosine similarity)\n",
    "    vectors = []\n",
    "    for i in range(num_vectors):\n",
    "        vec = np.random.randn(dimension).astype(np.float32)\n",
    "        vec = vec / np.linalg.norm(vec)  # Normalize\n",
    "        vectors.append(vec.tolist())\n",
    "    \n",
    "    # Generate IDs and metadata - FIXED: Ensure same length\n",
    "    ids = [f\"test_doc_{i}\" for i in range(num_vectors)]\n",
    "    \n",
    "    metadatas = []\n",
    "    sample_titles = [\n",
    "        \"Machine Learning Research Paper\",\n",
    "        \"Deep Learning Applications\", \n",
    "        \"Natural Language Processing\",\n",
    "        \"Computer Vision Advances\",\n",
    "        \"Reinforcement Learning\"\n",
    "    ]\n",
    "    \n",
    "    # FIXED: Generate metadata for ALL vectors, not just first 5\n",
    "    for i in range(num_vectors):\n",
    "        title = sample_titles[i % len(sample_titles)]  # Cycle through titles if num_vectors > 5\n",
    "        metadatas.append({\n",
    "            \"title\": f\"{title} #{i}\",\n",
    "            \"authors\": [f\"Author_{i}\", f\"Researcher_{i % 5}\"],\n",
    "            \"year\": 2020 + (i % 5),\n",
    "            \"abstract\": f\"This is a test abstract for {title} - document {i}\",\n",
    "            \"doc_id\": f\"doc_{i}\",\n",
    "            \"is_public\": i % 2 == 0  # Alternate public/private\n",
    "        })\n",
    "    \n",
    "    print(f\"  âœ… Generated {len(vectors)} embeddings, {len(ids)} IDs, {len(metadatas)} metadata entries\")\n",
    "    return vectors, ids, metadatas\n",
    "\n",
    "\n",
    "def generate_sample_embeddings_OLD(num_vectors: int = 5, dimension: int = 384) -> tuple:\n",
    "    \"\"\"Generate sample embeddings for testing\"\"\"\n",
    "    print(f\"\\nðŸ§ª Test 2: Generating {num_vectors} sample embeddings...\")\n",
    "    \n",
    "    # Generate random vectors (normalized for cosine similarity)\n",
    "    vectors = []\n",
    "    for i in range(num_vectors):\n",
    "        vec = np.random.randn(dimension).astype(np.float32)\n",
    "        vec = vec / np.linalg.norm(vec)  # Normalize\n",
    "        vectors.append(vec.tolist())\n",
    "    \n",
    "    # Generate IDs and metadata\n",
    "    ids = [f\"test_doc_{i}\" for i in range(num_vectors)]\n",
    "    \n",
    "    metadatas = []\n",
    "    sample_titles = [\n",
    "        \"Machine Learning Research Paper\",\n",
    "        \"Deep Learning Applications\", \n",
    "        \"Natural Language Processing\",\n",
    "        \"Computer Vision Advances\",\n",
    "        \"Reinforcement Learning\"\n",
    "    ]\n",
    "    \n",
    "    for i, title in enumerate(sample_titles):\n",
    "        metadatas.append({\n",
    "            \"title\": title,\n",
    "            \"authors\": [f\"Author_{i}\", f\"Researcher_{i}\"],\n",
    "            \"year\": 2020 + i,\n",
    "            \"abstract\": f\"This is a test abstract for {title}\",\n",
    "            \"doc_id\": f\"doc_{i}\",\n",
    "            \"is_public\": i % 2 == 0  # Alternate public/private\n",
    "        })\n",
    "    \n",
    "    print(f\"âœ… Generated {len(vectors)} embeddings with dimension {len(vectors[0])}\")\n",
    "    return vectors, ids, metadatas\n",
    "\n",
    "# Generate sample data\n",
    "vectors, ids, metadatas = generate_sample_embeddings()\n",
    "print(f\"Sample vector: {vectors[0][:5]}...\")  # Show first 5 dimensions\n",
    "print(f\"Sample metadata: {metadatas[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c6a5f12d-555f-456b-9ebc-9a3b82e596d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Debug: Checking vector generation...\n",
      "âŒ BROKEN: vectors: 20, ids: 20, metadatas: 5\n",
      "âœ… FIXED: vectors: 20, ids: 20, metadatas: 20\n"
     ]
    }
   ],
   "source": [
    "# Debug: Check lengths before running performance test\n",
    "def debug_length_check():\n",
    "    print(\"ðŸ” Debug: Checking vector generation...\")\n",
    "    \n",
    "    # Test the original broken function\n",
    "    def original_broken_function(num_vectors=20):\n",
    "        vectors = [f\"vector_{i}\" for i in range(num_vectors)]\n",
    "        ids = [f\"id_{i}\" for i in range(num_vectors)]\n",
    "        \n",
    "        metadatas = []\n",
    "        sample_titles = [\"Title1\", \"Title2\", \"Title3\", \"Title4\", \"Title5\"]\n",
    "        for i, title in enumerate(sample_titles):\n",
    "            metadatas.append({\"title\": title})\n",
    "            \n",
    "        return vectors, ids, metadatas\n",
    "    \n",
    "    vectors, ids, metadatas = original_broken_function(20)\n",
    "    print(f\"âŒ BROKEN: vectors: {len(vectors)}, ids: {len(ids)}, metadatas: {len(metadatas)}\")\n",
    "    \n",
    "    # Test the fixed function\n",
    "    def fixed_function(num_vectors=20):\n",
    "        vectors = [f\"vector_{i}\" for i in range(num_vectors)]\n",
    "        ids = [f\"id_{i}\" for i in range(num_vectors)]\n",
    "        \n",
    "        metadatas = []\n",
    "        sample_titles = [\"Title1\", \"Title2\", \"Title3\", \"Title4\", \"Title5\"]\n",
    "        for i in range(num_vectors):\n",
    "            title = sample_titles[i % len(sample_titles)]\n",
    "            metadatas.append({\"title\": f\"{title}_{i}\"})\n",
    "            \n",
    "        return vectors, ids, metadatas\n",
    "    \n",
    "    vectors, ids, metadatas = fixed_function(20)\n",
    "    print(f\"âœ… FIXED: vectors: {len(vectors)}, ids: {len(ids)}, metadatas: {len(metadatas)}\")\n",
    "\n",
    "debug_length_check()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819d08c5-137e-45ae-babd-3eaa8766491f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Test 3: Upsert/Insert Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29a9c9fc-0cee-4205-a562-9a502acede74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ§ª Test 3: Testing Upsert Operations...\n",
      "\n",
      "ðŸ“¦ Upserting to namespace: user_123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.services.vector_db.local_vector_db:Upserted batch of 5 vectors to Firestore\n",
      "INFO:src.services.vector_db.local_vector_db:Upserted batch of 5 vectors to Firestore\n",
      "INFO:src.services.vector_db.local_vector_db:Upserted batch of 5 vectors to Firestore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  âœ… Successfully upserted: 5 vectors\n",
      "  âœ… No errors\n",
      "\n",
      "ðŸ“¦ Upserting to namespace: public\n",
      "  âœ… Successfully upserted: 5 vectors\n",
      "  âœ… No errors\n",
      "\n",
      "ðŸ“¦ Upserting to namespace: test_namespace\n",
      "  âœ… Successfully upserted: 5 vectors\n",
      "  âœ… No errors\n",
      "\n",
      "ðŸ“Š Final collection stats: {'vectors_count': 15, 'namespaces': ['test_namespace', 'user_123', 'public'], 'shards_count': 1, 'collection_name': 'vectors'}\n"
     ]
    }
   ],
   "source": [
    "# Test 3: Upsert/Insert Test\n",
    "async def test_upsert():\n",
    "    print(f\"\\nðŸ§ª Test 3: Testing Upsert Operations...\")\n",
    "    \n",
    "    # Test with different namespaces\n",
    "    namespaces = [\"user_123\", \"public\", \"test_namespace\"]\n",
    "    \n",
    "    for namespace in namespaces:\n",
    "        print(f\"\\nðŸ“¦ Upserting to namespace: {namespace}\")\n",
    "        \n",
    "        try:\n",
    "            results = await vector_db.upsert_embeddings(\n",
    "                vectors=vectors,\n",
    "                ids=[f\"{namespace}_{id}\" for id in ids],  # Make IDs namespace-specific\n",
    "                metadatas=metadatas,\n",
    "                namespace=namespace\n",
    "            ) \n",
    "            \n",
    "            # Check results\n",
    "            successes = [r for r in results if r[\"status\"] == \"success\"]\n",
    "            errors = [r for r in results if r[\"status\"] == \"error\"]\n",
    "            \n",
    "            print(f\"  âœ… Successfully upserted: {len(successes)} vectors\")\n",
    "            if errors:\n",
    "                print(f\"  âŒ Errors: {len(errors)}\")\n",
    "                for error in errors:\n",
    "                    print(f\"    - {error}\")\n",
    "            else:\n",
    "                print(f\"  âœ… No errors\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  âŒ Upsert failed for namespace {namespace}: {e}\")\n",
    "    \n",
    "    # Check final stats\n",
    "    stats = await vector_db.get_index_stats()\n",
    "    print(f\"\\nðŸ“Š Final collection stats: {stats}\")\n",
    "\n",
    "# Run test\n",
    "await test_upsert()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbbc5e96-6716-4f0c-a3d1-6f2e54ef1605",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Test 4: Search Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "142849ff-0f15-4243-9498-0e9f9df456e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ§ª Test 4: Testing Search Operations...\n",
      "\n",
      "ðŸ” Test 4.1: Search across all namespaces\n",
      "  Found 3 results:\n",
      "    1. ID: public_test_doc_0, Distance: 0.0000\n",
      "       Title: Machine Learning Research Paper\n",
      "    2. ID: test_namespace_test_doc_0, Distance: 0.0000\n",
      "       Title: Machine Learning Research Paper\n",
      "    3. ID: user_123_test_doc_0, Distance: 0.0000\n",
      "       Title: Machine Learning Research Paper\n",
      "\n",
      "ðŸ” Test 4.2: Search within 'user_123' namespace\n",
      "  Found 2 results in namespace 'user_123':\n",
      "    1. ID: user_123_test_doc_0, Distance: 0.0000\n",
      "    2. ID: user_123_test_doc_4, Distance: 0.9278\n",
      "\n",
      "ðŸ” Test 4.3: Search with metadata filters\n",
      "  Found 0 public documents:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/google/cloud/firestore_v1/base_collection.py:304: UserWarning: Detected filter using positional arguments. Prefer using the 'filter' keyword argument instead.\n",
      "  return query.where(field_path, op_string, value)\n"
     ]
    }
   ],
   "source": [
    "async def test_search():\n",
    "    print(f\"\\nðŸ§ª Test 4: Testing Search Operations...\")\n",
    "    \n",
    "    # Use the first vector as query\n",
    "    query_vector = vectors[0]\n",
    "    \n",
    "    # Test 4.1: Search across all namespaces\n",
    "    print(f\"\\nðŸ” Test 4.1: Search across all namespaces\")\n",
    "    try:\n",
    "        results = await vector_db.search(\n",
    "            query_embedding=query_vector,\n",
    "            top_k=3,\n",
    "            namespace=None  # Search all namespaces\n",
    "        )\n",
    "        \n",
    "        print(f\"  Found {len(results)} results:\")\n",
    "        for i, result in enumerate(results):\n",
    "            print(f\"    {i+1}. ID: {result['id']}, Distance: {result['distance']:.4f}\")\n",
    "            print(f\"       Title: {result['metadata'].get('title', 'N/A')}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"  âŒ Search failed: {e}\")\n",
    "    \n",
    "    # Test 4.2: Search within specific namespace\n",
    "    print(f\"\\nðŸ” Test 4.2: Search within 'user_123' namespace\")\n",
    "    try:\n",
    "        results = await vector_db.search(\n",
    "            query_embedding=query_vector,\n",
    "            top_k=2,\n",
    "            namespace=\"user_123\"\n",
    "        )\n",
    "        \n",
    "        print(f\"  Found {len(results)} results in namespace 'user_123':\")\n",
    "        for i, result in enumerate(results):\n",
    "            print(f\"    {i+1}. ID: {result['id']}, Distance: {result['distance']:.4f}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"  âŒ Namespace search failed: {e}\")\n",
    "    \n",
    "    # Test 4.3: Search with filters\n",
    "    print(f\"\\nðŸ” Test 4.3: Search with metadata filters\")\n",
    "    try:\n",
    "        results = await vector_db.search(\n",
    "            query_embedding=query_vector,\n",
    "            top_k=5,\n",
    "            namespace=None,\n",
    "            filter={\"is_public\": [True]}  # Only public documents\n",
    "        )\n",
    "        \n",
    "        print(f\"  Found {len(results)} public documents:\")\n",
    "        for i, result in enumerate(results):\n",
    "            is_public = result['metadata'].get('is_public', False)\n",
    "            print(f\"    {i+1}. ID: {result['id']}, Public: {is_public}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"  âŒ Filtered search failed: {e}\")\n",
    "\n",
    "# Run test\n",
    "await test_search()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77537b53-e191-425b-ad8d-5a0cf015719c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Test 5: Delete Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66bed259-03f5-4822-be9d-0600d3104915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ§ª Test 5: Testing Delete Operations...\n",
      "ðŸ—‘ï¸  Deleting 2 documents: ['user_123_test_doc_0', 'user_123_test_doc_1']\n",
      "  âœ… Successfully deleted: 2 documents\n",
      "\n",
      "ðŸ” Verifying deletion by searching...\n",
      "  Remaining documents in 'user_123': 3\n",
      "  Remaining IDs: ['user_123_test_doc_4', 'user_123_test_doc_3', 'user_123_test_doc_2']\n",
      "  âœ… Document user_123_test_doc_0 successfully deleted\n",
      "  âœ… Document user_123_test_doc_1 successfully deleted\n"
     ]
    }
   ],
   "source": [
    "async def test_delete():\n",
    "    print(f\"\\nðŸ§ª Test 5: Testing Delete Operations...\")\n",
    "    \n",
    "    # Delete documents from user_123 namespace\n",
    "    docs_to_delete = [f\"user_123_{id}\" for id in ids[:2]]  # Delete first 2 docs\n",
    "    \n",
    "    print(f\"ðŸ—‘ï¸  Deleting {len(docs_to_delete)} documents: {docs_to_delete}\")\n",
    "    \n",
    "    try:\n",
    "        delete_results = await vector_db.delete(docs_to_delete)\n",
    "        \n",
    "        successes = [r for r in delete_results if r[\"status\"] == \"success\"]\n",
    "        errors = [r for r in delete_results if r[\"status\"] == \"error\"]\n",
    "        \n",
    "        print(f\"  âœ… Successfully deleted: {len(successes)} documents\")\n",
    "        if errors:\n",
    "            print(f\"  âŒ Delete errors: {len(errors)}\")\n",
    "            for error in errors:\n",
    "                print(f\"    - {error}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"  âŒ Delete operation failed: {e}\")\n",
    "    \n",
    "    # Verify deletion by searching\n",
    "    print(f\"\\nðŸ” Verifying deletion by searching...\")\n",
    "    try:\n",
    "        query_vector = vectors[0]\n",
    "        results = await vector_db.search(\n",
    "            query_embedding=query_vector,\n",
    "            top_k=10,\n",
    "            namespace=\"user_123\"\n",
    "        )\n",
    "        \n",
    "        remaining_ids = [r[\"id\"] for r in results]\n",
    "        print(f\"  Remaining documents in 'user_123': {len(remaining_ids)}\")\n",
    "        print(f\"  Remaining IDs: {remaining_ids}\")\n",
    "        \n",
    "        # Check if deleted docs are gone\n",
    "        for deleted_id in docs_to_delete:\n",
    "            if deleted_id in remaining_ids:\n",
    "                print(f\"  âš ï¸  Document {deleted_id} still exists!\")\n",
    "            else:\n",
    "                print(f\"  âœ… Document {deleted_id} successfully deleted\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"  âŒ Verification search failed: {e}\")\n",
    "\n",
    "# Run test\n",
    "await test_delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24dea0b4-acd9-43b1-b1b2-7c624f6a807a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Test 6: Performance Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed0385b5-019b-482f-8a93-8985c30cc467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ§ª Test 6: Testing Performance...\n",
      "\n",
      "ðŸ§ª Test 2: Generating 20 sample embeddings...\n",
      "âœ… Generated 20 embeddings with dimension 384\n",
      "â±ï¸  Testing with 20 vectors...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Vectors, IDs, and metadatas must have the same length",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 44\u001b[39m\n\u001b[32m     41\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  ðŸ§¹ Cleaned up performance test data\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     43\u001b[39m \u001b[38;5;66;03m# Run test\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m test_performance()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 16\u001b[39m, in \u001b[36mtest_performance\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Test upsert performance\u001b[39;00m\n\u001b[32m     15\u001b[39m start_time = time.time()\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m results = \u001b[38;5;28;01mawait\u001b[39;00m vector_db.upsert_embeddings(\n\u001b[32m     17\u001b[39m     vectors=large_vectors,\n\u001b[32m     18\u001b[39m     ids=[\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mperf_test_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mid\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mid\u001b[39m \u001b[38;5;129;01min\u001b[39;00m large_ids],\n\u001b[32m     19\u001b[39m     metadatas=large_metadatas,\n\u001b[32m     20\u001b[39m     namespace=\u001b[33m\"\u001b[39m\u001b[33mperformance_test\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     21\u001b[39m )\n\u001b[32m     22\u001b[39m upsert_time = time.time() - start_time\n\u001b[32m     24\u001b[39m successes = \u001b[38;5;28mlen\u001b[39m([r \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m results \u001b[38;5;28;01mif\u001b[39;00m r[\u001b[33m\"\u001b[39m\u001b[33mstatus\u001b[39m\u001b[33m\"\u001b[39m] == \u001b[33m\"\u001b[39m\u001b[33msuccess\u001b[39m\u001b[33m\"\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/app/src/services/vector_db/local_vector_db.py:47\u001b[39m, in \u001b[36mLocalVectorDBService.upsert_embeddings\u001b[39m\u001b[34m(self, vectors, ids, metadatas, namespace)\u001b[39m\n\u001b[32m     45\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Upsert embeddings into Firestore with vector support.\"\"\"\u001b[39;00m\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(vectors) != \u001b[38;5;28mlen\u001b[39m(ids) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(vectors) != \u001b[38;5;28mlen\u001b[39m(metadatas):\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m     48\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mVectors, IDs, and metadatas must have the same length\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     50\u001b[39m batch_size = settings.matching_engine_batch_size \u001b[38;5;129;01mor\u001b[39;00m \u001b[32m100\u001b[39m\n\u001b[32m     51\u001b[39m results = []\n",
      "\u001b[31mValueError\u001b[39m: Vectors, IDs, and metadatas must have the same length"
     ]
    }
   ],
   "source": [
    "# Test 6: Performance Test - FIXED VERSION\n",
    "async def test_performance():\n",
    "    print(f\"\\nðŸ§ª Test 6: Testing Performance...\")\n",
    "    \n",
    "    # FIXED: Generate consistent number of vectors, ids, and metadatas\n",
    "    def generate_large_sample_embeddings(num_vectors: int = 20, dimension: int = 384) -> tuple:\n",
    "        \"\"\"Generate sample embeddings for performance testing\"\"\"\n",
    "        print(f\"  Generating {num_vectors} sample embeddings...\")\n",
    "        \n",
    "        # Generate random vectors (normalized for cosine similarity)\n",
    "        vectors = []\n",
    "        for i in range(num_vectors):\n",
    "            vec = np.random.randn(dimension).astype(np.float32)\n",
    "            vec = vec / np.linalg.norm(vec)  # Normalize\n",
    "            vectors.append(vec.tolist())\n",
    "        \n",
    "        # Generate IDs and metadata - FIXED: Ensure same length\n",
    "        ids = [f\"test_doc_{i}\" for i in range(num_vectors)]\n",
    "        \n",
    "        metadatas = []\n",
    "        sample_titles = [\n",
    "            \"Machine Learning Research Paper\",\n",
    "            \"Deep Learning Applications\", \n",
    "            \"Natural Language Processing\",\n",
    "            \"Computer Vision Advances\",\n",
    "            \"Reinforcement Learning\",\n",
    "            \"Neural Networks Study\",\n",
    "            \"AI Ethics and Society\",\n",
    "            \"Quantum Computing Research\",\n",
    "            \"Data Mining Techniques\",\n",
    "            \"Robotics and Automation\"\n",
    "        ]\n",
    "        \n",
    "        # FIXED: Generate metadata for ALL vectors, not just first 5\n",
    "        for i in range(num_vectors):\n",
    "            title = sample_titles[i % len(sample_titles)]  # Cycle through titles\n",
    "            metadatas.append({\n",
    "                \"title\": f\"{title} #{i}\",\n",
    "                \"authors\": [f\"Author_{i}\", f\"Researcher_{i % 5}\"],\n",
    "                \"year\": 2020 + (i % 5),\n",
    "                \"abstract\": f\"This is a test abstract for {title} - document {i}\",\n",
    "                \"doc_id\": f\"doc_{i}\",\n",
    "                \"is_public\": i % 2 == 0  # Alternate public/private\n",
    "            })\n",
    "        \n",
    "        print(f\"  âœ… Generated {len(vectors)} embeddings, {len(ids)} IDs, {len(metadatas)} metadata entries\")\n",
    "        return vectors, ids, metadatas\n",
    "\n",
    "    # Generate larger batch with FIXED function\n",
    "    large_vectors, large_ids, large_metadatas = generate_large_sample_embeddings(\n",
    "        num_vectors=20, \n",
    "        dimension=384\n",
    "    )\n",
    "    \n",
    "    print(f\"â±ï¸  Testing with {len(large_vectors)} vectors...\")\n",
    "    \n",
    "    import time\n",
    "    \n",
    "    # Test upsert performance\n",
    "    start_time = time.time()\n",
    "    results = await vector_db.upsert_embeddings(\n",
    "        vectors=large_vectors,\n",
    "        ids=[f\"perf_test_{id}\" for id in large_ids],\n",
    "        metadatas=large_metadatas,\n",
    "        namespace=\"performance_test\"\n",
    "    )\n",
    "    upsert_time = time.time() - start_time\n",
    "    \n",
    "    successes = len([r for r in results if r[\"status\"] == \"success\"])\n",
    "    print(f\"  ðŸ“ˆ Upsert Performance: {successes} docs in {upsert_time:.2f}s \"\n",
    "          f\"({successes/upsert_time:.1f} docs/sec)\")\n",
    "    \n",
    "    # Test search performance\n",
    "    start_time = time.time()\n",
    "    search_results = await vector_db.search(\n",
    "        query_embedding=large_vectors[0],\n",
    "        top_k=10,\n",
    "        namespace=\"performance_test\"\n",
    "    )\n",
    "    search_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"  ðŸ“ˆ Search Performance: {len(search_results)} results in {search_time:.4f}s\")\n",
    "    \n",
    "    # Verify search results\n",
    "    if search_results:\n",
    "        print(f\"  ðŸ” Best match: {search_results[0]['id']} (distance: {search_results[0]['distance']:.4f})\")\n",
    "    \n",
    "    # Clean up performance test data\n",
    "    delete_results = await vector_db.delete([f\"perf_test_{id}\" for id in large_ids])\n",
    "    delete_successes = len([r for r in delete_results if r[\"status\"] == \"success\"])\n",
    "    print(f\"  ðŸ§¹ Cleaned up {delete_successes} performance test documents\")\n",
    "\n",
    "# Run test\n",
    "await test_performance()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64a1be6-530a-49b6-b5f2-08d6752b976d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Test 7: Edge Cases Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2b689aa-a2e8-42c2-9461-c642e2b7e3df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.services.vector_db.local_vector_db:Upserted batch of 2 vectors to Firestore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ§ª Test 7: Testing Edge Cases...\n",
      "\n",
      "ðŸ” Test 7.1: Empty inputs\n",
      "  âœ… Empty upsert handled: []\n",
      "\n",
      "ðŸ” Test 7.2: Similar vectors search\n",
      "  Similar vectors distance: 0.018226\n",
      "  âœ… Similar vectors test completed\n",
      "\n",
      "ðŸ” Test 7.3: Non-existent namespace\n",
      "  Search in non-existent namespace returned: 0 results\n",
      "  âœ… Non-existent namespace handled correctly\n"
     ]
    }
   ],
   "source": [
    "# Test 7: Edge Cases Test\n",
    "async def test_edge_cases():\n",
    "    print(f\"\\nðŸ§ª Test 7: Testing Edge Cases...\")\n",
    "    \n",
    "    # Test 7.1: Empty inputs\n",
    "    print(f\"\\nðŸ” Test 7.1: Empty inputs\")\n",
    "    try:\n",
    "        results = await vector_db.upsert_embeddings([], [], [])\n",
    "        print(f\"  âœ… Empty upsert handled: {results}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  âŒ Empty upsert failed: {e}\")\n",
    "    \n",
    "    # Test 7.2: Very similar vectors\n",
    "    print(f\"\\nðŸ” Test 7.2: Similar vectors search\")\n",
    "    try:\n",
    "        # Create two very similar vectors\n",
    "        base_vector = np.random.randn(384).astype(np.float32)\n",
    "        base_vector = base_vector / np.linalg.norm(base_vector)\n",
    "        \n",
    "        similar_vector = base_vector + 0.01 * np.random.randn(384)\n",
    "        similar_vector = similar_vector / np.linalg.norm(similar_vector)\n",
    "        \n",
    "        # Store them\n",
    "        await vector_db.upsert_embeddings(\n",
    "            vectors=[base_vector.tolist(), similar_vector.tolist()],\n",
    "            ids=[\"similar_1\", \"similar_2\"],\n",
    "            metadatas=[{\"test\": \"similar_1\"}, {\"test\": \"similar_2\"}],\n",
    "            namespace=\"similarity_test\"\n",
    "        )\n",
    "        \n",
    "        # Search and check distance\n",
    "        results = await vector_db.search(\n",
    "            query_embedding=base_vector.tolist(),\n",
    "            top_k=2,\n",
    "            namespace=\"similarity_test\"\n",
    "        )\n",
    "        \n",
    "        print(f\"  Similar vectors distance: {results[1]['distance']:.6f}\")\n",
    "        print(f\"  âœ… Similar vectors test completed\")\n",
    "        \n",
    "        # Clean up\n",
    "        await vector_db.delete([\"similar_1\", \"similar_2\"])\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  âŒ Similar vectors test failed: {e}\")\n",
    "    \n",
    "    # Test 7.3: Non-existent namespace search\n",
    "    print(f\"\\nðŸ” Test 7.3: Non-existent namespace\")\n",
    "    try:\n",
    "        results = await vector_db.search(\n",
    "            query_embedding=vectors[0],\n",
    "            top_k=5,\n",
    "            namespace=\"non_existent_namespace_12345\"\n",
    "        )\n",
    "        print(f\"  Search in non-existent namespace returned: {len(results)} results\")\n",
    "        print(f\"  âœ… Non-existent namespace handled correctly\")\n",
    "    except Exception as e:\n",
    "        print(f\"  âŒ Non-existent namespace search failed: {e}\")\n",
    "\n",
    "# Run test\n",
    "await test_edge_cases()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c0870d-3f94-4f27-8dfe-fb20b2c458b1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Final Summary and Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b369263-6039-4d63-950f-b30e171f207e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "ðŸ“Š FINAL TEST SUMMARY\n",
      "==================================================\n",
      "Final collection stats: {'vectors_count': 13, 'namespaces': ['test_namespace', 'user_123', 'public'], 'shards_count': 1, 'collection_name': 'vectors'}\n",
      "\n",
      "ðŸ“‚ Documents by namespace:\n",
      "  user_123: 3 documents\n",
      "  public: 5 documents\n",
      "  test_namespace: 5 documents\n",
      "  performance_test: 0 documents\n",
      "  similarity_test: 0 documents\n",
      "\n",
      "ðŸŽ¯ Test Suite Completed!\n",
      "âœ… Firestore Local Vector DB is working correctly\n",
      "ðŸ’¡ Use VECTOR_DB_TYPE=local in your environment to enable this service\n"
     ]
    }
   ],
   "source": [
    "# Final Summary and Cleanup\n",
    "async def final_summary():\n",
    "    print(f\"\\n\" + \"=\"*50)\n",
    "    print(f\"ðŸ“Š FINAL TEST SUMMARY\")\n",
    "    print(f\"=\"*50)\n",
    "    \n",
    "    # Get final stats\n",
    "    stats = await vector_db.get_index_stats()\n",
    "    print(f\"Final collection stats: {stats}\")\n",
    "    \n",
    "    # Count documents by namespace\n",
    "    print(f\"\\nðŸ“‚ Documents by namespace:\")\n",
    "    namespaces = [\"user_123\", \"public\", \"test_namespace\", \"performance_test\", \"similarity_test\"]\n",
    "    \n",
    "    for namespace in namespaces:\n",
    "        try:\n",
    "            results = await vector_db.search(\n",
    "                query_embedding=vectors[0],\n",
    "                top_k=100,  # Large number to count all\n",
    "                namespace=namespace\n",
    "            )\n",
    "            print(f\"  {namespace}: {len(results)} documents\")\n",
    "        except:\n",
    "            print(f\"  {namespace}: 0 documents (or error)\")\n",
    "    \n",
    "    print(f\"\\nðŸŽ¯ Test Suite Completed!\")\n",
    "    print(f\"âœ… Firestore Local Vector DB is working correctly\")\n",
    "    print(f\"ðŸ’¡ Use VECTOR_DB_TYPE=local in your environment to enable this service\")\n",
    "\n",
    "# Run final summary\n",
    "await final_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5734e7e-499c-453b-b4e2-0966f713e4da",
   "metadata": {},
   "source": [
    "# Quick health check function for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "89b0ea80-b7ec-4631-8905-af0b7f56ee0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ¥ Running Quick Health Check...\n",
      "ðŸŸ¢ Vector DB Healthy - Total vectors: 13\n"
     ]
    }
   ],
   "source": [
    "# Quick health check function for future use\n",
    "async def health_check():\n",
    "    \"\"\"Quick health check for the vector DB\"\"\"\n",
    "    try:\n",
    "        stats = await vector_db.get_index_stats()\n",
    "        print(f\"ðŸŸ¢ Vector DB Healthy - Total vectors: {stats.get('vectors_count', 0)}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"ðŸ”´ Vector DB Unhealthy: {e}\")\n",
    "        return False\n",
    "\n",
    "# Run quick health check\n",
    "print(f\"\\nðŸ¥ Running Quick Health Check...\")\n",
    "is_healthy = await health_check()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776ed411-9319-4578-b5ed-dfc1205a2441",
   "metadata": {},
   "source": [
    "# Miscs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d4238283-9da7-4f10-90e8-7db123e6d038",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vectors_count': 13,\n",
       " 'namespaces': ['test_namespace', 'user_123', 'public'],\n",
       " 'shards_count': 1,\n",
       " 'collection_name': 'vectors'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await vector_db.get_index_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d5974440-8dcc-415f-9f3e-7881c0b596cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method upsert_embeddings in module src.services.vector_db.local_vector_db:\n",
      "\n",
      "async upsert_embeddings(vectors: List[List[float]], ids: List[str], metadatas: List[Dict[str, Any]], namespace: Optional[str] = None) method of src.services.vector_db.local_vector_db.LocalVectorDBService instance\n",
      "    Upsert embeddings into Firestore with vector support.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(vector_db.upsert_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "649102ff-13e9-40c0-b437-c9ddd6a7a558",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.services.vector_db.local_vector_db:Upserted batch of 1 vectors to Firestore\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'status': 'success', 'id': 'abc5'}]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await vector_db.upsert_embeddings(vectors=[[.35]*384,], \n",
    "                                  ids=['abc5',], \n",
    "                                  metadatas=[{'a': 2, 'b': 3}], \n",
    "                                  namespace='user007')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0e527c3c-e9e9-4cf7-9b8d-cd90435408a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vectors_count': 18,\n",
       " 'namespaces': ['public', 'test_namespace', 'user_123', 'user007'],\n",
       " 'shards_count': 1,\n",
       " 'collection_name': 'vectors'}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await vector_db.get_index_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c5c8a01c-9353-417e-bcda-1c3d5a0902f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method search in module src.services.vector_db.local_vector_db:\n",
      "\n",
      "async search(query_embedding: List[float], top_k: int, namespace: Optional[str] = None, filter: Optional[Dict[str, Any]] = None) -> List[Dict[str, Any]] method of src.services.vector_db.local_vector_db.LocalVectorDBService instance\n",
      "    Search for similar vectors using cosine similarity.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(vector_db.search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "cbe3a260-c79d-49d1-8723-4e0027de307d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 'abc1', 'distance': np.float64(0.0), 'metadata': {'b': 3, 'a': 2}},\n",
       " {'id': 'abc2', 'distance': np.float64(0.0), 'metadata': {'b': 3, 'a': 2}},\n",
       " {'id': 'abc4',\n",
       "  'distance': np.float64(1.1102230246251565e-16),\n",
       "  'metadata': {'b': 3, 'a': 2}},\n",
       " {'id': 'abc5',\n",
       "  'distance': np.float64(2.220446049250313e-16),\n",
       "  'metadata': {'b': 3, 'a': 2}},\n",
       " {'id': 'abc3', 'distance': np.float64(2.0), 'metadata': {'b': 3, 'a': 2}}]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await vector_db.search(query_embedding=[0.1]*384,\n",
    "                       top_k=10,\n",
    "                       namespace=\"user007\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781dae79-67a9-4e57-87d3-c867bd5bff16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
